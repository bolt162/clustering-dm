{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# DBSCAN Clustering using PyCaret\n",
    "\n",
    "## Assignment Part D\n",
    "\n",
    "This notebook demonstrates DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering using the PyCaret library.\n",
    "\n",
    "### What is DBSCAN?\n",
    "DBSCAN is a density-based clustering algorithm that:\n",
    "- Groups together points that are closely packed together\n",
    "- Marks points in low-density regions as outliers\n",
    "- Can find arbitrarily shaped clusters\n",
    "- Does not require specifying the number of clusters beforehand\n",
    "\n",
    "### Why PyCaret?\n",
    "PyCaret is a low-code machine learning library that simplifies the clustering workflow with:\n",
    "- Automated preprocessing\n",
    "- Multiple clustering algorithms\n",
    "- Built-in evaluation metrics\n",
    "- Visualization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": "# Install required libraries with proper dependency handling\n# First, uninstall and reinstall numpy to ensure compatibility\n!pip uninstall numpy -y -q\n!pip install numpy==1.23.5 -q\n!pip install pycaret -q\n!pip install plotly -q\n!pip install scikit-learn pandas matplotlib seaborn -q\n\nprint(\"Installation complete!\")\nprint(\"Note: If you still see warnings, restart the runtime (Runtime -> Restart runtime) and run again.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from pycaret.clustering import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "We'll use multiple datasets to demonstrate DBSCAN's strength in finding non-spherical clusters:\n",
    "1. Customer segmentation data from online\n",
    "2. Synthetic datasets (moons, circles) to show DBSCAN's capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_real_data"
   },
   "outputs": [],
   "source": [
    "# Load customer segmentation dataset from online source\n",
    "url = \"https://raw.githubusercontent.com/dphi-official/Datasets/master/Mall_Customers.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_viz"
   },
   "outputs": [],
   "source": [
    "# Visualize the data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(data['Age'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Age Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Income distribution\n",
    "axes[0, 1].hist(data['Annual Income (k$)'], bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Annual Income Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Annual Income (k$)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Spending Score distribution\n",
    "axes[1, 0].hist(data['Spending Score (1-100)'], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Spending Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Spending Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Income vs Spending Score scatter\n",
    "axes[1, 1].scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], alpha=0.6)\n",
    "axes[1, 1].set_title('Income vs Spending Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Annual Income (k$)')\n",
    "axes[1, 1].set_ylabel('Spending Score (1-100)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pycaret_setup"
   },
   "source": [
    "## 3. PyCaret Setup\n",
    "\n",
    "Initialize the PyCaret clustering environment. This step handles:\n",
    "- Data preprocessing\n",
    "- Feature scaling\n",
    "- Missing value treatment\n",
    "- Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Select features for clustering\n",
    "# We'll use Annual Income and Spending Score for clear visualization\n",
    "clustering_data = data[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Setup PyCaret environment\n",
    "cluster_setup = setup(\n",
    "    data=clustering_data,\n",
    "    normalize=True,  # Normalize features\n",
    "    session_id=123,  # For reproducibility\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"PyCaret setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_training"
   },
   "source": [
    "## 4. Create and Train DBSCAN Model\n",
    "\n",
    "### DBSCAN Parameters:\n",
    "- **eps (epsilon)**: Maximum distance between two samples for them to be considered in the same neighborhood\n",
    "- **min_samples**: Minimum number of samples in a neighborhood for a point to be considered a core point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# Create DBSCAN model with PyCaret\n",
    "dbscan_model = create_model(\n",
    "    'dbscan',\n",
    "    num_clusters=None  # DBSCAN determines clusters automatically\n",
    ")\n",
    "\n",
    "print(\"\\nDBSCAN Model created successfully!\")\n",
    "print(f\"Model parameters: {dbscan_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "assign_clusters"
   },
   "outputs": [],
   "source": [
    "# Assign clusters to the data\n",
    "result = assign_model(dbscan_model)\n",
    "\n",
    "print(\"\\nClustering results:\")\n",
    "display(result.head(10))\n",
    "\n",
    "# Check cluster distribution\n",
    "print(\"\\nCluster distribution:\")\n",
    "print(result['Cluster'].value_counts().sort_index())\n",
    "\n",
    "# Note: Cluster -1 represents noise/outliers in DBSCAN\n",
    "n_clusters = len(result['Cluster'].unique()) - (1 if -1 in result['Cluster'].values else 0)\n",
    "n_noise = list(result['Cluster']).count(-1)\n",
    "\n",
    "print(f\"\\nNumber of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points (outliers): {n_noise}\")\n",
    "print(f\"Percentage of outliers: {(n_noise/len(result))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## 5. Visualize Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_model"
   },
   "outputs": [],
   "source": [
    "# Use PyCaret's built-in visualization\n",
    "print(\"Cluster Visualization:\")\n",
    "plot_model(dbscan_model, plot='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_distribution"
   },
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "print(\"Cluster Distribution:\")\n",
    "plot_model(dbscan_model, plot='distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_viz"
   },
   "outputs": [],
   "source": [
    "# Custom detailed visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot 1: Clusters with different colors\n",
    "scatter = axes[0].scatter(\n",
    "    result['Annual Income (k$)'],\n",
    "    result['Spending Score (1-100)'],\n",
    "    c=result['Cluster'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black'\n",
    ")\n",
    "axes[0].set_xlabel('Annual Income (k$)', fontsize=12)\n",
    "axes[0].set_ylabel('Spending Score (1-100)', fontsize=12)\n",
    "axes[0].set_title('DBSCAN Clustering Results', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot 2: Highlight outliers\n",
    "outliers = result[result['Cluster'] == -1]\n",
    "non_outliers = result[result['Cluster'] != -1]\n",
    "\n",
    "axes[1].scatter(\n",
    "    non_outliers['Annual Income (k$)'],\n",
    "    non_outliers['Spending Score (1-100)'],\n",
    "    c=non_outliers['Cluster'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black',\n",
    "    label='Clusters'\n",
    ")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    axes[1].scatter(\n",
    "        outliers['Annual Income (k$)'],\n",
    "        outliers['Spending Score (1-100)'],\n",
    "        c='red',\n",
    "        s=150,\n",
    "        alpha=0.8,\n",
    "        marker='x',\n",
    "        edgecolors='black',\n",
    "        linewidths=2,\n",
    "        label='Outliers'\n",
    "    )\n",
    "\n",
    "axes[1].set_xlabel('Annual Income (k$)', fontsize=12)\n",
    "axes[1].set_ylabel('Spending Score (1-100)', fontsize=12)\n",
    "axes[1].set_title('DBSCAN with Outliers Highlighted', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Clustering Quality Evaluation\n",
    "\n",
    "We'll evaluate the clustering using multiple metrics:\n",
    "- **Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters (-1 to 1, higher is better)\n",
    "- **Davies-Bouldin Index**: Average similarity ratio of each cluster with its most similar cluster (lower is better)\n",
    "- **Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster dispersion (higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics"
   },
   "outputs": [],
   "source": [
    "# Prepare data for evaluation (exclude outliers for some metrics)\n",
    "non_outlier_data = result[result['Cluster'] != -1]\n",
    "X_eval = non_outlier_data[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
    "labels_eval = non_outlier_data['Cluster'].values\n",
    "\n",
    "# Calculate clustering metrics\n",
    "if len(non_outlier_data['Cluster'].unique()) > 1:\n",
    "    silhouette = silhouette_score(X_eval, labels_eval)\n",
    "    davies_bouldin = davies_bouldin_score(X_eval, labels_eval)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_eval, labels_eval)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CLUSTERING QUALITY METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nSilhouette Score: {silhouette:.4f}\")\n",
    "    print(\"  → Range: [-1, 1], Higher is better\")\n",
    "    print(\"  → Interpretation: Measures how similar points are to their own cluster\")\n",
    "    \n",
    "    print(f\"\\nDavies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    print(\"  → Range: [0, ∞], Lower is better\")\n",
    "    print(\"  → Interpretation: Average similarity between each cluster and its most similar one\")\n",
    "    \n",
    "    print(f\"\\nCalinski-Harabasz Score: {calinski_harabasz:.4f}\")\n",
    "    print(\"  → Range: [0, ∞], Higher is better\")\n",
    "    print(\"  → Interpretation: Ratio of between-cluster to within-cluster dispersion\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Silhouette Score', 'Davies-Bouldin Index', 'Calinski-Harabasz Score'],\n",
    "        'Value': [silhouette, davies_bouldin, calinski_harabasz],\n",
    "        'Interpretation': ['Higher is better', 'Lower is better', 'Higher is better']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    display(metrics_df)\n",
    "else:\n",
    "    print(\"Only one cluster found. Metrics cannot be calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cluster_stats"
   },
   "outputs": [],
   "source": [
    "# Detailed cluster statistics\n",
    "print(\"\\nDetailed Cluster Statistics:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in sorted(result['Cluster'].unique()):\n",
    "    cluster_data = result[result['Cluster'] == cluster_id]\n",
    "    \n",
    "    if cluster_id == -1:\n",
    "        print(f\"\\nOUTLIERS (Cluster {cluster_id}):\")\n",
    "    else:\n",
    "        print(f\"\\nCLUSTER {cluster_id}:\")\n",
    "    \n",
    "    print(f\"  Size: {len(cluster_data)} points ({len(cluster_data)/len(result)*100:.2f}%)\")\n",
    "    print(f\"  Income - Mean: ${cluster_data['Annual Income (k$)'].mean():.2f}k, \"\n",
    "          f\"Std: ${cluster_data['Annual Income (k$)'].std():.2f}k\")\n",
    "    print(f\"  Spending - Mean: {cluster_data['Spending Score (1-100)'].mean():.2f}, \"\n",
    "          f\"Std: {cluster_data['Spending Score (1-100)'].std():.2f}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "synthetic_demo"
   },
   "source": [
    "## 7. DBSCAN on Synthetic Datasets\n",
    "\n",
    "Demonstrate DBSCAN's strength in finding non-spherical clusters using synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "synthetic_data"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Moons dataset (non-spherical)\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# 2. Circles dataset (non-spherical)\n",
    "X_circles, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
    "\n",
    "# 3. Blobs dataset (spherical)\n",
    "X_blobs, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)\n",
    "\n",
    "datasets = [\n",
    "    ('Moons', X_moons),\n",
    "    ('Circles', X_circles),\n",
    "    ('Blobs', X_blobs)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "for idx, (name, X) in enumerate(datasets):\n",
    "    # Original data\n",
    "    axes[0, idx].scatter(X[:, 0], X[:, 1], alpha=0.6, s=50)\n",
    "    axes[0, idx].set_title(f'{name} - Original Data', fontsize=14, fontweight='bold')\n",
    "    axes[0, idx].set_xlabel('Feature 1')\n",
    "    axes[0, idx].set_ylabel('Feature 2')\n",
    "    \n",
    "    # DBSCAN clustering\n",
    "    df_temp = pd.DataFrame(X, columns=['feature1', 'feature2'])\n",
    "    \n",
    "    # Setup and cluster\n",
    "    s = setup(data=df_temp, normalize=True, session_id=123, verbose=False)\n",
    "    model = create_model('dbscan', num_clusters=None, verbose=False)\n",
    "    result_temp = assign_model(model)\n",
    "    \n",
    "    # Plot clustered data\n",
    "    scatter = axes[1, idx].scatter(\n",
    "        result_temp['feature1'],\n",
    "        result_temp['feature2'],\n",
    "        c=result_temp['Cluster'],\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        s=50,\n",
    "        edgecolors='black'\n",
    "    )\n",
    "    axes[1, idx].set_title(f'{name} - DBSCAN Clustering', fontsize=14, fontweight='bold')\n",
    "    axes[1, idx].set_xlabel('Feature 1')\n",
    "    axes[1, idx].set_ylabel('Feature 2')\n",
    "    plt.colorbar(scatter, ax=axes[1, idx], label='Cluster')\n",
    "    \n",
    "    # Print cluster info\n",
    "    n_clusters = len(result_temp['Cluster'].unique()) - (1 if -1 in result_temp['Cluster'].values else 0)\n",
    "    n_noise = list(result_temp['Cluster']).count(-1)\n",
    "    print(f\"{name}: {n_clusters} clusters, {n_noise} outliers\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuning"
   },
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "\n",
    "Experiment with different eps and min_samples values to optimize clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tune_model"
   },
   "outputs": [],
   "source": [
    "# Tune DBSCAN with different parameters\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Reset setup with original data\n",
    "s = setup(data=clustering_data, normalize=True, session_id=123, verbose=False)\n",
    "\n",
    "# Try different parameter combinations\n",
    "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
    "min_samples_values = [3, 5, 10]\n",
    "\n",
    "results_tuning = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        # Create DBSCAN with custom parameters\n",
    "        dbscan_custom = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        \n",
    "        # Fit and predict\n",
    "        X_normalized = get_config('X_train')\n",
    "        labels = dbscan_custom.fit_predict(X_normalized)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # Calculate silhouette if possible\n",
    "        if n_clusters > 1 and n_noise < len(labels):\n",
    "            non_noise_idx = labels != -1\n",
    "            if sum(non_noise_idx) > 0:\n",
    "                sil_score = silhouette_score(X_normalized[non_noise_idx], labels[non_noise_idx])\n",
    "            else:\n",
    "                sil_score = -1\n",
    "        else:\n",
    "            sil_score = -1\n",
    "        \n",
    "        results_tuning.append({\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'noise_pct': (n_noise/len(labels))*100,\n",
    "            'silhouette': sil_score\n",
    "        })\n",
    "\n",
    "# Display tuning results\n",
    "tuning_df = pd.DataFrame(results_tuning)\n",
    "tuning_df = tuning_df.sort_values('silhouette', ascending=False)\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "print(\"=\"*80)\n",
    "display(tuning_df)\n",
    "\n",
    "# Find best parameters\n",
    "best_params = tuning_df.iloc[0]\n",
    "print(f\"\\nBest Parameters:\")\n",
    "print(f\"  eps: {best_params['eps']}\")\n",
    "print(f\"  min_samples: {best_params['min_samples']}\")\n",
    "print(f\"  Silhouette Score: {best_params['silhouette']:.4f}\")\n",
    "print(f\"  Number of Clusters: {int(best_params['n_clusters'])}\")\n",
    "print(f\"  Noise Points: {int(best_params['n_noise'])} ({best_params['noise_pct']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## 9. Compare DBSCAN with Other Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_models"
   },
   "outputs": [],
   "source": [
    "# Reset setup\n",
    "s = setup(data=clustering_data, normalize=True, session_id=123, verbose=False)\n",
    "\n",
    "# Create multiple clustering models\n",
    "print(\"Creating and comparing different clustering algorithms...\\n\")\n",
    "\n",
    "# K-Means\n",
    "kmeans = create_model('kmeans', num_clusters=5, verbose=False)\n",
    "kmeans_result = assign_model(kmeans)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = create_model('dbscan', verbose=False)\n",
    "dbscan_result = assign_model(dbscan)\n",
    "\n",
    "# Hierarchical Clustering\n",
    "hclust = create_model('hclust', num_clusters=5, verbose=False)\n",
    "hclust_result = assign_model(hclust)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(\n",
    "    kmeans_result['Annual Income (k$)'],\n",
    "    kmeans_result['Spending Score (1-100)'],\n",
    "    c=kmeans_result['Cluster'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black'\n",
    ")\n",
    "axes[0].set_title('K-Means Clustering', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Annual Income (k$)')\n",
    "axes[0].set_ylabel('Spending Score (1-100)')\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# DBSCAN\n",
    "scatter2 = axes[1].scatter(\n",
    "    dbscan_result['Annual Income (k$)'],\n",
    "    dbscan_result['Spending Score (1-100)'],\n",
    "    c=dbscan_result['Cluster'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black'\n",
    ")\n",
    "axes[1].set_title('DBSCAN Clustering', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Annual Income (k$)')\n",
    "axes[1].set_ylabel('Spending Score (1-100)')\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "# Hierarchical\n",
    "scatter3 = axes[2].scatter(\n",
    "    hclust_result['Annual Income (k$)'],\n",
    "    hclust_result['Spending Score (1-100)'],\n",
    "    c=hclust_result['Cluster'],\n",
    "    cmap='viridis',\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    edgecolors='black'\n",
    ")\n",
    "axes[2].set_title('Hierarchical Clustering', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Annual Income (k$)')\n",
    "axes[2].set_ylabel('Spending Score (1-100)')\n",
    "plt.colorbar(scatter3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare metrics\n",
    "comparison_data = []\n",
    "for name, result_df in [('K-Means', kmeans_result), ('DBSCAN', dbscan_result), ('Hierarchical', hclust_result)]:\n",
    "    non_outlier = result_df[result_df['Cluster'] != -1]\n",
    "    X_eval = non_outlier[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
    "    labels_eval = non_outlier['Cluster'].values\n",
    "    \n",
    "    if len(non_outlier['Cluster'].unique()) > 1:\n",
    "        sil = silhouette_score(X_eval, labels_eval)\n",
    "        db = davies_bouldin_score(X_eval, labels_eval)\n",
    "        ch = calinski_harabasz_score(X_eval, labels_eval)\n",
    "    else:\n",
    "        sil, db, ch = -1, -1, -1\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Algorithm': name,\n",
    "        'Clusters': len(result_df['Cluster'].unique()),\n",
    "        'Silhouette': sil,\n",
    "        'Davies-Bouldin': db,\n",
    "        'Calinski-Harabasz': ch\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nAlgorithm Comparison:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **DBSCAN Advantages:**\n",
    "   - Does not require specifying the number of clusters\n",
    "   - Can find arbitrarily shaped clusters\n",
    "   - Robust to outliers (identifies them explicitly)\n",
    "   - Works well with non-spherical cluster shapes\n",
    "\n",
    "2. **DBSCAN Limitations:**\n",
    "   - Sensitive to eps and min_samples parameters\n",
    "   - Struggles with varying density clusters\n",
    "   - Not suitable for high-dimensional data (curse of dimensionality)\n",
    "\n",
    "3. **When to Use DBSCAN:**\n",
    "   - Unknown number of clusters\n",
    "   - Non-spherical cluster shapes\n",
    "   - Need to identify outliers\n",
    "   - Varying cluster sizes\n",
    "\n",
    "4. **PyCaret Benefits:**\n",
    "   - Simplified workflow with automatic preprocessing\n",
    "   - Built-in visualization tools\n",
    "   - Easy model comparison\n",
    "   - Consistent API across different algorithms\n",
    "\n",
    "### Clustering Quality Summary:\n",
    "The evaluation metrics help us understand:\n",
    "- **Silhouette Score**: How well-separated the clusters are\n",
    "- **Davies-Bouldin Index**: Compactness and separation of clusters\n",
    "- **Calinski-Harabasz Score**: Overall cluster quality\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different eps and min_samples values\n",
    "- Try HDBSCAN for varying density clusters\n",
    "- Apply dimensionality reduction before clustering high-dimensional data\n",
    "- Use domain knowledge to interpret and validate clusters"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DBSCAN_PyCaret_Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}